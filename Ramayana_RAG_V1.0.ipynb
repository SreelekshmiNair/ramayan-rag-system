{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0d6327ff6544e15aad55f7036113acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ad8c6a0e4e34e4788754f9f27dedd07",
              "IPY_MODEL_50180ef5036d41db8f26f04cea451a8a",
              "IPY_MODEL_af9122a72bfc4a1ea725f8fc2dd0bfc7"
            ],
            "layout": "IPY_MODEL_1993f896444a4dd0aec054c94a8f3091"
          }
        },
        "6ad8c6a0e4e34e4788754f9f27dedd07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55ae90dcbc684dd687bc383a29c31c32",
            "placeholder": "​",
            "style": "IPY_MODEL_f9ad707e3ad940bc959fe3f62a983bfe",
            "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
          }
        },
        "50180ef5036d41db8f26f04cea451a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06b98528c70c44a9bf54f1a5b1af4885",
            "max": 5942065440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ebae528cb2a4c8791f3bcb222dd4894",
            "value": 5942065440
          }
        },
        "af9122a72bfc4a1ea725f8fc2dd0bfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a373afc498540a8bac4379923585a2e",
            "placeholder": "​",
            "style": "IPY_MODEL_c21476c0fd3f4f2998ecad01b4a650a5",
            "value": " 5.94G/5.94G [01:19&lt;00:00, 274MB/s]"
          }
        },
        "1993f896444a4dd0aec054c94a8f3091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ae90dcbc684dd687bc383a29c31c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ad707e3ad940bc959fe3f62a983bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06b98528c70c44a9bf54f1a5b1af4885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ebae528cb2a4c8791f3bcb222dd4894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a373afc498540a8bac4379923585a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21476c0fd3f4f2998ecad01b4a650a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a RAG System on Ramayana PDF\n",
        "\n",
        "### Goals of this Notebook\n",
        "\n",
        "We will:\n",
        "\n",
        "Load an LLM model and check the response generated without help from RAG. Then,\n",
        "\n",
        "1. Load a Ramayana PDF document  \n",
        "2. Convert it into searchable text  \n",
        "3. Create vector embeddings  \n",
        "4. Build a Retrieval Augmented Generation (RAG) pipeline  \n",
        "5. Ask questions to a model **without RAG**  \n",
        "6. Ask the same questions **with RAG**  \n",
        "7. Evaluate the quality improvement  \n",
        "\n"
      ],
      "metadata": {
        "id": "-_2B3ldLq9kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ramayana RAG System – Using Mistral (Local LLM)\n",
        "\n",
        "This notebook builds a full Retrieval Augmented Generation system using:\n",
        "\n",
        "- Mistral 7B (local via llama-cpp)\n",
        "- Chroma Vector Database\n",
        "- Sentence Transformer embeddings\n",
        "- PyMuPDF PDF loader"
      ],
      "metadata": {
        "id": "X5VdkhCcsmi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZA3db31q7kM",
        "outputId": "016c8b26-5083-41f0-8783-2cac8d36a237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.13 requires numpy<2,>=1, but you have numpy 2.4.2 which is incompatible.\n",
            "langchain 0.1.1 requires numpy<2,>=1, but you have numpy 2.4.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
            "google-adk 1.24.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is being used\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For installing the libraries & downloading models from HF Hub\n",
        "!pip install --upgrade pip -q\n",
        "\n",
        "!pip install \\\n",
        "huggingface_hub \\\n",
        "pandas \\\n",
        "tiktoken \\\n",
        "pymupdf \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "chromadb \\\n",
        "sentence-transformers \\\n",
        "llama-cpp-python -q\n"
      ],
      "metadata": {
        "id": "WSzdwp6nrU8N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries for processing dataframes,text\n",
        "import json,os\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#Libraries for downloading and loading the llm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "H7hDPsEyrozh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering using LLM"
      ],
      "metadata": {
        "id": "SiucNQVJGHHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and Loading the model"
      ],
      "metadata": {
        "id": "VMk2aSdNGKez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Mistral model\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"  # the model is in gguf format"
      ],
      "metadata": {
        "id": "RoP4MNwsGHzW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using hf_hub_download to download a model from the Hugging Face model hub\n",
        "# The repo_id parameter specifies the model name or path in the Hugging Face repository\n",
        "# The filename parameter specifies the name of the file to download\n",
        "model_path = hf_hub_download(\n",
        "    repo_id= model_name_or_path, #code to mention the repo id\n",
        "    filename= model_basename #code to mention the model name\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "a0d6327ff6544e15aad55f7036113acf",
            "6ad8c6a0e4e34e4788754f9f27dedd07",
            "50180ef5036d41db8f26f04cea451a8a",
            "af9122a72bfc4a1ea725f8fc2dd0bfc7",
            "1993f896444a4dd0aec054c94a8f3091",
            "55ae90dcbc684dd687bc383a29c31c32",
            "f9ad707e3ad940bc959fe3f62a983bfe",
            "06b98528c70c44a9bf54f1a5b1af4885",
            "9ebae528cb2a4c8791f3bcb222dd4894",
            "8a373afc498540a8bac4379923585a2e",
            "c21476c0fd3f4f2998ecad01b4a650a5"
          ]
        },
        "id": "DXketqEmGNbj",
        "outputId": "a12ded9d-4f44-4a1c-a237-8733f5700944"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0d6327ff6544e15aad55f7036113acf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model with GPU support\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2300, # Context window\n",
        "    n_gpu_layers=38,\n",
        "    n_batch=512\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Rx8j6HGVIm",
        "outputId": "fbce74f9-9827-4a7b-d432-56b2a4c5ca2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Response"
      ],
      "metadata": {
        "id": "ClC7gmAvGXey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate responses from the LLM\n",
        "#response function creates a reusable function to generate responses from the LLM\n",
        "#- Handles all inference parameters in one place\n",
        "#- Returns just the text response\n",
        "def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    model_output = llm(\n",
        "      prompt=query,\n",
        "      max_tokens=max_tokens, #Maximum number of tokens to generate\n",
        "      temperature=temperature, #Controls randomness\n",
        "      top_p=top_p, #picks from top tokens that make up top_p of total probability\n",
        "      top_k=top_k #considers only the top_k most likely tokens\n",
        "    )\n",
        "\n",
        "    return model_output['choices'][0]['text']"
      ],
      "metadata": {
        "id": "0gNIFv6lGYPa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sample Questions\n",
        "\n",
        "\n",
        "    1. Who is Hanuman and what role does he play in the Ramayana?\n",
        "    2. What happened in the battle between Rama and Ravana?\n",
        "    3. Tell me about Sita's character in the Ramayana.\n",
        "    4. Who is Queen Tara and what happens to her?"
      ],
      "metadata": {
        "id": "DJYJnrgiG4Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing sample questions into variables for easse of usage\n",
        "qn1 = \"Who is Hanuman and what role does he play in the Ramayana?\"\n",
        "qn2 = \"What happened in the battle between Rama and Ravana?\"\n",
        "qn3 = \"Tell me about Sita's character in the Ramayana.\"\n",
        "qn4 = \"Who is Queen Tara and what happens to her?\""
      ],
      "metadata": {
        "id": "Z_I9Ltx2HOBD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question1\n",
        "response(qn1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xHopd_SqHL_X",
        "outputId": "2eaa2f10-ac73-46fb-ae97-f92a8aa2050e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nHanuman is a central character in the Indian epic Ramayana, which narrates the adventures of Prince Rama, an incarnation of the Hindu god Vishnu. Hanuman is known as the monkey god or the vanara god and is revered for his devotion to Lord Rama, his strength, and his intelligence.\\n\\nHanuman was born to Anjani, a vanara (monkey) princess, and the wind god, Vayu. He grew up in the forest with other monkeys and apes. Hanuman is best known for his unwavering'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question2\n",
        "response(qn2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "uZtqZ1-zJUNR",
        "outputId": "303d2df4-7229-4f48-fc4a-794136ad3388"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nRavana, the king of Lanka, was a powerful demon king who had abducted Sita, wife of Rama. Rama, along with his brother Lakshmana, set out on a journey to rescue Sita. They reached the forest of Dandaka where they met Sage Valmiki and received blessings from him.\\n\\nRavana, who was aware of Rama's mission, sent his brother Vibhishana to invite Rama for peace talks. Rama agreed but on the condition that Sita should be present during the talks. Ravana agreed\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question3\n",
        "response(qn3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "obE17RsgJVyX",
        "outputId": "4a301d9f-5dfd-4c6b-b147-f824bae132a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nSita, also known as Janaki or Seetha, is one of the most revered and beloved characters in Hindu mythology. She is the wife of Lord Rama, an avatar of Vishnu, and is considered an ideal woman and a paragon of virtue and devotion.\\n\\nAccording to the epic Ramayana, Sita was born in the kingdom of Mithila to King Janaka and Queen Sunanda. She was discovered by sage Valmiki while she was playing in the forest as a child, and he predicted that she would one day become the wife of an avatar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question4\n",
        "response(qn4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "0PLYkCofJWGO",
        "outputId": "ec8855f5-8a6b-4ab4-ca26-c3c4fdf3284b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nQueen Tara, also known as Taranis, is a character in the 1982 animated film The Dark Crystal. She is the queen of the Gelfling tribe of Em-Kai and the wife of King Kermit. When the Skeksis gain control of the Crystal of Truth, they use it to corrupt the Saplings, which are the source of life for the Gelflings. Queen Tara becomes ill as a result of this corruption and eventually dies.\\n\\nThe main character of the story, Kermit the Gelfling, sets out on a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations on the Model**\n",
        "\n",
        "- This establishes our baseline performance of the **Mistral language model** without any additional prompt engineering or retrieval augmentation. The intent was to understand how effectively the base model alone could answer Ramayana queries drawn from data out there.\n",
        "- The model generated relevant and contextually coherent responses except for the last query where it answered the question out of context.The responses reflected general understanding and followed a logical flow.\n",
        "- While the content was somwhat accurate except for one, the responses were incomplete, often stopping mid-sentence and lacked depth and was generic.\n",
        "- The model generally delivers useful information; however, its responses are often high-level, making them appear more like general answers than context-specific response."
      ],
      "metadata": {
        "id": "M6wQENGwLx3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering using LLM with Prompt Engineering"
      ],
      "metadata": {
        "id": "lsV0K40DMSBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining system prompt ####\n",
        "\n",
        "- Adds structure and guidance to LLM responses\n",
        "- Sets expectations for medical accuracy\n",
        "- Improves response format and quality\n",
        "- Still no external context, just better instructions"
      ],
      "metadata": {
        "id": "6l1p07BMMUam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating response function for 5 different combinations ####\n",
        "\n",
        "Considering **response1** function as the base function, creating 4 other combinations and comparing it with response function\n",
        "reponse1 has parameters\n",
        "\n",
        " **max_tokens** = 1024,   **temp** = 1.1   **top_p** = 0.95  **top_k** = 50\n",
        "\n",
        "- **response2** - changed temp value from 0.7 to 0.0\n",
        "- **response3** - changed top_p value from 0.95 to 0.85\n",
        "- **response4** - changed top_k value from 50 to 80\n",
        "- **response5** - changed max_tokens from 1024 to 512"
      ],
      "metadata": {
        "id": "G6em5CIOMXJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating system instruction\n",
        "system_message = \"\"\"\"\"\"  #code to define the system prompt\n",
        "\n",
        "#Creeate system prompt with instructions for the model\n",
        "#system_prompt = f\"[INST]<<SYS>>\\n{system_message}\\n<</SYS>>[/INST]\"\n",
        "system_prompt = f\"[INST]<<SYS>>\\n{system_message}\\n<</SYS>>\\n\"\n"
      ],
      "metadata": {
        "id": "u7YUq1txMi-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C_Crp0-QWP3V"
      }
    }
  ]
}