Overview

This project implements a GPU-accelerated Retrieval-Augmented Generation (RAG) system built on the Ramayan corpus.
It enables contextual question answering by combining semantic search with a locally hosted Mistral LLM from Hugging Face.
The system retrieves relevant passages from the Ramayan and generates grounded responses using a large language model.

ðŸ“š Complete RAG System for Ramayana - Beginner Friendly Guide

ðŸŽ¯ What You'll Learn
- What RAG (Retrieval Augmented Generation) is and why it's powerful
- How to build a complete RAG system from scratch using FREE tools
- How to evaluate your RAG system
- How to fine-tune models for better performance
- Before/After comparisons to see the improvement

---

ðŸ¤” What is RAG?

**RAG (Retrieval Augmented Generation)** combines two powerful capabilities:

1. **Retrieval**: Finding relevant information from a knowledge base (Ramayana PDF)
2. **Generation**: Using an LLM to generate accurate answers based on that retrieved information

**Why is this better than just asking an LLM directly?**
- âœ… **Up-to-date info**: LLMs have knowledge cutoffs; RAG uses YOUR documents
- âœ… **Accuracy**: Answers are grounded in actual source material. 
- âœ… **Citations**: You can show which part of the document the answer came from
- âœ… **Domain-specific**: Works with specialized knowledge (like Ramayana)

**The RAG Pipeline:**
User Question â†’ Retrieve Relevant Passages â†’ LLM Generates Answer â†’ Response

What we're installing and why:

PyMuPDF: Extracts text from PDF files
sentence-transformers: Creates embeddings (vector representations of text)
chromadb: Vector database to store and search embeddings
transformers: Hugging Face library for using LLMs
langchain: Framework for building RAG applications - FREE!
